\documentclass{uit-thesis}

\usepackage{enumitem}
\usepackage{/usr/local/texlive/2019/texmf-dist/tex/latex/tlatex/tlatex}
\usepackage[backend=biber, sorting=none]{biblatex}
\addbibresource{sources.bib}
\graphicspath{{./figures/}}

\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
% \renewcommand{\implies}{\rightarrow}
\newcommand{\eventually}{\rightsquigarrow}

\usepackage{color}
\definecolor{boxshade}{gray}{0.85}
\setboolean{shading}{true}

\begin{document}

\title{Formal Verification of a Lock-free Split-order Hashmap}
% \subtitle{Subtitle}% Note: this is optional, and may be commented out
\author{Åsmund Aqissiaq Arild Kløvstad}
\thesisfaculty{Faculty of Science and Technology \\ Department of Computer Science}
\thesisprogramme{INF-2990 Bachelor's thesis in Informatics \today{}}

\maketitle

\frontmatter

\begin{epigraph}
\epigraphitem{You don't need to understand everything at once. You understand one thing, then you pat yourself on the back, have a cup of coffee, and understand one more thing.}{Nada Amin}
\end{epigraph}

\tableofcontents

\mainmatter

\chapter{Introduction}
Concurrent and distributed systems are extremely important in modern software development. Due to the difficulty of developing ever smaller and more powerful CPUs the trend in hardware design since about 2005 has been to increase the number of cores to allow for high levels of parallelization~\cite{Tanenbaum2014}. Additionally important areas of computing such as image processing and machine learning lend themselves well to such parallelization. [citation needed, Phuong?] At the same time software as a service and the massive scale industry giants like Amazon require a complex network of distributed systems to provide their functionality robustly and efficiently~\cite{Amazon2015}.
\\\\
Hash tables are an important data structure for a variety of applications because they allow for data retrieval in constant time. Several lock-based hash tables for concurrent systems exist [citations], but the overhead of lock management and difficulty of resizing often make these impractical or inefficient~\cite{Shalev2006}. A lock free alternative is proposed by Shalev and Shavit in \cite{Shalev2006}. This approach has proven to be useful~\cite{lock-free-structures2013} and scale better with number of concurrent processes than lock-based approaches~\cite{Duarte-Haskell2016}.
\\\\
In both small- and large-scale computer systems it is important to ensure correctness. This is especially evident in critical infrastructure, but all scales and importance levels benefit from confidence in the correctness of their systems. [citation needed]
\\\\
It is therefore troublesome that such systems are incredibly difficult to design, debug and reason about. The complexity of interactions between processes and sheer number of possible edge cases makes it infeasible for a person to determine correctness.
\\\\
Early solutions to the problem of proving correctness include Hoare~\cite{Hoare1969}, Floyd~\cite{Floyd1967} and Pnueli's~\cite{Pnueli1977} temporal logics and Leslie Lamport's Temporal Logic of Actions~\cite{Lamport1977} which seek to formalize the execution of programs in order to reason about them with logic. These formal methods proved useful, but laborious~\cite{Clarke2009}.
\\\\
Building on the work in temporal logics, model checkers seek to minimize the human labor and ingenuity needed to prove correctness. This is done by specifying a model using some system of logic and then letting a model checker exhaustively survey the possible states of the system. This automates the process of proving correctness. One such model checker is the TLC model checker based on Lamport's TLA and incorporated in the TLA+ IDE.

\section{Thesis}
Shalev et al.'s split-ordered list design is a correct extensible hashmap for concurrent systems.
\\\\
Furthermore it is possible to check this using a formal model checker, and the results of this will correspond to the properties proven by Shalev et al.

\section{Method}
In order to prove the correctness of the hashmap, its behavior will be implemented as a specification in TLA+~\cite{Lamport_specifying_2002} and the TLC model checker will be used to test the claimed invariants.
\\\\
The specification will be developed in stages of increasing granularity, assuming the atomicity of operations to begin with and gradually loosening assumptions.

\section{Scope?}

\section{Outline}
\begin{enumerate}[label={}]
    \item \textbf{\Autoref{ch:background}} discusses the motivation for formal verification and model checking, followed by a description of Temporal Logic in \autoref{sec:TL} and the TLA+ language in \autoref{sec:TLA+}. Finally Shalev et al.'s hashmap design is described in \autoref{sec:hashmap}.
    \item \textbf{\Autoref{ch:specification}} describes the specification of the hashmap in TLA+ and the development of this specification.
    \item Finally \textbf{\autoref{ch:results}} describes the results of model checking and discusses these in relation to our thesis in \autoref{sec:discussion}. 
\end{enumerate}


\chapter{Background}\label{ch:background}
\paragraph{Model Checking: Algorithmic Verification and Debugging~\cite{Clarke2009}}
In the Turing Lecture by the winners of the 2007 Turing Award, Edmund Clarke, Allen Emerson and Joseph Sifakis they describe the develpment and use of model checkers as a verification method for computer systems. Previous efforts to prove correctness had been focused on formal proofs which have three key shortcomings:
\begin{enumerate}
    \item they require human ingenuity,
    \item they are difficult to work with in concurrent and distributed systems,
    \item they scale poorly with system size and complexity.
\end{enumerate}
Instead, they propose algorithmic model checkers.
\\\\
With this method a Temporal Logic is used to specify the correct behavior of a system and the model checker verifies that this behavior is not violated by exploring the state space of the model. Importantly, such model checkers produce a counter example -- an example of incorrect behavior -- which makes debugging and correcting the system easier.
Key properties of a temporal logic are \textit{expressiveness} and \textit{efficiency}.
\\\\
Model checking also scales poorly with system complexity, so several techniques are introduced to deal with "state space explosion"
\begin{itemize}
    \item symbolic checking of ordered binary decision diagrams
    \item isolation of independent events in concurrent systems
    \item bounded checking by solving SAT
    \item reduce state space by increasing level of abstraction
    \begin{itemize}
        \item if counterexamples are found a lower abstraction level is needed, but "good" properties hold through abstraction mappings
    \end{itemize}
\end{itemize}


\paragraph{How Amazon Web Services Uses Formal Methods~\cite{Amazon2015}}
Amazon's AWS services are all underpinned by large and complex distributed systems. This is necessary for high availability, growth and cost-effective infrastructure. Traditionally these systems have been tested by savvy engineers who know what to test and look for. However, some errors are very rare and will very likely slip through such testing. To catch these errors they employ model checking (with TLA+).
\\\\
The PlusCal or TLA+ specifications work as a tool to bridge the gap between design and implementation. Designs are expressive, but inprecise while the implementation is precise, but hides overall structure. Through a choice of abstraction level, specifications can bridge this gap and provide both. An expressive specification also provides useful documentation of the system.
\\\\
The key benefits of model checkers at Amazon are:
\begin{itemize}
    \item a precisely specified design helps make changes and optimizations safely. This usage improves system understanding.
    \item they are faster than formal proofs
    \item a correct design and the understanding the specs provide promote better, more correct code.
\end{itemize}

\section{Temporal Logics}\label{sec:TL}
\paragraph{The Temporal Logic of Programs ~\cite{Pnueli1977}}
In The Temporal Logic of Programs~\cite{Pnueli1977}, Amir Pnueli proposes a unified approach to the verification of both sequential and concurrent programs. His work seeks to unify approaches to to both, while also presenting a system that emulates the design intuition of programmers. The key concepts in this work are \textit{invariance} -- which covers partial correctness, clean behavior, mutual exlusion and deadlock freedom -- and \textit{eventuality} -- which generalizes these notions to cyclic programs and provides a special case of total correctness.
\\\\
A dynamic discrete system is generalized as a three-tuple $\langle S, R, s_0 \rangle$ where $S$ is the set of possible states, $R$ a transition relation, and $s_0$ the initial state of the system. In order to make later constructions easier we further specify
$$s = \langle \pi, u \rangle$$ where $\pi$ is the control component specifying the location in the program and $u$ is the data component describing the state of any variables and data structures, and
$$R(\pi, u) = N(\pi, u) \land T(\pi, u)$$ where $N$ describes the control flow and T the change in data such that a step in the execution may be described by
$$R(\langle \pi, u \rangle , \langle \pi', u' \rangle) \iff \pi' = N(\pi, u) \land u' = T(\pi, u)$$
To reason about concurrent programs we let states have multiple control components $s = \langle \pi_1, \pi_2,...,\pi_n, u \rangle$ and randomly choose one control component to update in each step. Finally we let $X$ be the set of all reachable states for the system. A predicate $p(s)$ is \textbf{invariant} if $p(s)$ is true $\forall s \in X$.
\\\\
We can now start to define useful properties of the systems described in this way.
\begin{itemize}[label={}]
    \item \textbf{Partial correctness} is the claim that given the correct input, a program produces the correct output. We let $\phi(x)$ be the statement "reaching the end state $\implies$ (correct input $\implies$ correct output)". Partial correctness is equivalent to saying $\phi$ is an invariant.
    \item \textbf{Clean execution} means the program does not behave illegally, i.e it does not access illegal memory locations or divide by zero. We may define these restrictions as a predicate to make clean execution equivalent to this predicate being invariant.
    \item \textbf{Mutual exclusion}. Given a critical section $C$, mutual exclusion of the processes $\pi_1$ and $\pi_2$ is described by the invariance of the predicate $\lnot(\pi_1 \in C \land \pi_2 \in C)$.
\end{itemize}
In addition to these properties we wish to reason about \textit{temporal} implications. We let time be described by a $t \in \N$ and $H(p,t)$ denote the value of the predicate $p$ at time $t$.
We then introduce the temporal operator $p \eventually q$ to mean $p$ eventually leads to $q$, or formally:
$$p \eventually q: \forall t_1 \exists t_2\ s.t\ t_1 \le t_2,  H(p, t_1) \implies H(q, t_2)$$
For all times $t_1$ there is a later time $t_2$ such that if $p$ holds at $t_1$, $q$ will hold at $t_2$.
Armed with eventuality we can define temporally useful properties of systems.
\begin{itemize}[label={}]
    \item \textbf{Total correctness} is stronger than partial correctness because it also requires that the program reaches an end state. We can express total correctness as $\langle \pi = l_0, u = \phi\rangle \eventually \langle \pi = l_m, u = \psi\rangle$ where $\phi$ denotes correct input, and $\psi$ denotes correct output and $l_0$, $l_m$ are the start and end labeles of the system, respectively.
    \item \textbf{Accessibility} is the guarantee that some segment $S$ of a program can be reached. It can be expressed by $\pi = l_0 \eventually \pi \in S$
    \item \textbf{Responsiveness}. It is often desirable that some request $r$ will be met by a response $s$. We call this responsiveness and describe it by $r \eventually s$.
\end{itemize}

With these definitions under our belt, Pnueli defines the necessary axioms and inference rules to reason about the correctness of programs.

\begin{figure}[h!]
    \begin{equation}\tag{A1}\label{axiom:1}
        [\forall s, s' p(s) \land R(s,s') \implies q(s')] \Rightarrow p \eventually q
    \end{equation}
    \begin{equation}\tag{A2}\label{axiom:2}
        (p \implies q) \Rightarrow p \eventually q
    \end{equation}
\caption{Pnueli's axioms}
\end{figure}

These axioms define two ways to establish eventuality. \ref{axiom:2} says that any logical implication is also an eventuality. \ref{axiom:1} is a little more involved, but states that if for all consecutive states $p$ being true in the first implies $q$ being true in the second, then $p$ eventually leads to $q$.

\begin{figure}[h]
    \begin{equation}\tag{R1}\label{rules:1}
        p \eventually q, \forall s, s' r(s) \land R(s,s') \implies r(s') \Rightarrow (p \land r) \eventually (q \land r)
    \end{equation}
    \begin{equation}\tag{R2}\label{rules:2}
        p \eventually q, q \eventually r \Rightarrow p \eventually r
    \end{equation}
    \begin{equation}\tag{R3}\label{rules:3}
        p_1 \eventually q, p_2 \eventually q \Rightarrow (p_1 \lor p_2) \eventually q
    \end{equation}
    \begin{equation}\tag{R4}\label{rules:4}
        p \eventually q \Rightarrow (\exists u p) \eventually q
    \end{equation}
\caption{Pnueli's inference rules}
\end{figure}

Using these axioms and inference rules as well as first-order logic, Pnueli goes on to formalize invariance and eventuality for sequential programs and concurrent programs.

\begin{itemize}[label={}]
    \item \textbf{Invariance} of the predicate $q(\pi, u)$ is described by the conjunction $\bigwedge\limits_{i}{\pi=l_i \implies q(l_i, u)}$, asserting that $q$ is true at all points of execution. This method is called an \textit{attachment} of the predicate to the program.
    \newpage{}
    In a concurrent program we generalize $q$ to hold when any $\pi_i$ is updated by $N$ and construct either a full attachment
    $$\bigwedge\limits_{i_1, i_2,...,i_n}{(\pi_1 = i_1 \land \pi_2 = i_2 \land ... \land \pi_n = i_n) \implies q(\pi_1,..,\pi_n,u)}$$ shown here for $n$ concurrent execution threads,
    or the partial attachment
    $$\bigwedge\limits_{i}{\pi_1=i \implies q(\pi_1 = i, \pi_2, u)} \land \bigwedge\limits_{j}{\pi_2=j \implies q(\pi_1, \pi_2 = j, u)}$$ shown here for two threads $\pi_1$ and $\pi_2$.
    \item \textbf{Eventuality} is formulated as the temporal implication $\pi = l_1 \land p(u) \eventually \pi=l_2 \land q(u)$. We can then describe the path between $l_1$ and $l_2$ by a finite sequence of steps and apply \ref{axiom:1} to each step.
\end{itemize}

Finally, Pnueli introduces two new "tense operators" \textbf{F}uture and \textbf{G}lobal on predicates such that at some time $n$
$$F(p) = \exists t \geq n \; s.t\; H(t,p)$$
$$G(p) = \forall t \geq n \; H(t,p)$$
This lets us describe useful properties such as
\begin{itemize}[label={}]
    \item $p \implies F(q)$ -- if $p$ is true now, then at some point in the future $q$ will be true.
    \item $G(p \implies F(q))$ -- whenever $p$ is true it will eventually be followed by a state in which $q$ is true. (this is equivalent to $p \eventually q$)     
\end{itemize}
\section{TLA+}\label{sec:TLA+}
%Needs to cover implementation of one spec by another
\section{Split-Ordered List Hashmap}\label{sec:hashmap}

Maybe a description of hashmaps in general here to set up the later use of table, list, key etc. or is that assumed knowledge?
\\\\
Shalev et al.~\cite{Shalev2006} present the first lock-free extensible hash table implemented using only loads, stores and atomic Compare and Swap (CAS).
\\\\
Hashmaps are a key building block in many important systems [citation needed], but are difficult to implement concurrently. In particular, the resizing (extending) of the table is difficult to do atomically because at the very least a node must be moved from one list to another. In order to avoid conflicts and loss of data in this process some overhead is required which impacts performance.
\\\\
The key insight of Shalev et al. is to flip the process upside down. Instead of moving nodes between buckets, they suggest moving the buckets among a statically ordered list of nodes. This requires an ordering of the list in which a bucket can always be split into two new buckets while their contents remain correct. A node should always reside in the bucket corresponding to its key $\bmod{2^i}$ where $2^i$ is the current size of the table.

\paragraph{Split-Ordered Lists}
are introduced to make resizing of the map possible without introducing locks. By sorting the keys according to their reversed binary representation Shalev et al. obtain a list which can be always be split into buckets $\bmod{2^i}$. This is because such an ordering corresponds to difference in the keys' $i$th least significant bit, which is equivalent to having a different remainder $\bmod{2^i}$.
\\\\
To deal with the problems caused by removing nodes pointed to by hash table entries dummy nodes with the bucket value are introduced. These nodes signify the start of a bucket and are recursively initialized when an item is inserted into an unitialized bucket. To distinguish dummy nodes from regular nodes in the list, regular node keys have their most significant bit set to 1 before being reversed. This order and the structure of the map can be seen in \autoref{fig:split-order}.
\\\\
Insertion in to the map is done through atomic CAS instructions on the list. If a bucket is not initialized, a dummy node is created and inserted into the list before the new value is added as shown in \autoref{fig:simple-insert}. The map is expanded by doubling the number of buckets and inserting new dummy nodes. Because of the split-ordering of the list, it is always possible to insert a new bucket by splitting an existing one. This process is shown in \autoref{fig:insert-expand}.
\begin{figure}
    \includegraphics[width=\textwidth]{split-ordered-list-diagram.png}
\caption{The layout of the split-ordered list}
\label{fig:split-order}
\end{figure}

\begin{figure}
    \centering
    \subfloat[Initial state]{\includegraphics[width=.5\textwidth]{insert-0.png}}
    \subfloat[Dummy node is created]{\includegraphics[width=.5\textwidth]{insert-1.png}}
    \newline
    \subfloat[Dummy node is inserted in bucket]{\includegraphics[width=.5\textwidth]{insert-2.png}}
    \subfloat[New node is inserted]{\includegraphics[width=.5\textwidth]{insert-3.png}}
\caption{Insertion without bucket splitting}    
\label{fig:simple-insert}
\end{figure}

\begin{figure}
    \centering
    \subfloat[Initial state]{\includegraphics[height=2in]{expand-0.png}}
    \newline
    \subfloat[Table is expanded]{\includegraphics[height=2in]{expand-1.png}}
    \newline
    \subfloat[Dummy node for new bucket inserted]{\includegraphics[height=2in]{expand-2.png}}
    \newline
    \subfloat[Table entry points to bucket node]{\includegraphics[height=2in]{expand-3.png}}
\caption{Expansion and bucket splitting}    
\label{fig:insert-expand}
\end{figure}


\chapter{Specification}\label{ch:specification}
%intro
% structure of this chapter, structure of the spec, scope
This chapter describes the structure of the TLA+ specification and the process of writing the specification. First we introduce the goals and scope of the specification, then describe the definitions of necessary data structures and operations.
\\\\
The specification consists of two main parts:
\begin{enumerate}
    \item a generic hashmap and
    \item an [insert name of structure here] \textit{implementing} this map.
\end{enumerate}
The generic specification describes the workings of a hashmap with insert and remove operations and ensures that this structure behaves as specified. The [insert name of structure] specification describes Shalev et al.'s specific structure and algorithms for implementing hashmap functionality.

\section{Goal}\label{sec:spec-goals}
The purpose of our specification is to verify the claims about correctness made by Shalev et al. Because TLA+ is designed for checking liveness and safety properties~\cite{Lund2019} we will not check claims about the performance of the implementation. This leaves 4 invariants:
\begin{itemize}
    \item the list beginning at bucket 0 is always sorted
    \item if a bucket is initialized, then it points to a dummy node which is in the list beginning at bucket 0
    \item if a key $k$ is in the map, then \texttt{insert(k)} fails. Otherwise $k$ is added to the map
    \item if a key $k$ is in the map, then \texttt{remove(k)} removes it from the map. Otherwise it fails.
\end{itemize}
[MIGHT CHANGE:] Rather than having functions return error codes, the generic specification describes the correct behavior of each. This allows us to check invariants by implementation of this generic specification, since implementing it correctly corresponds to correct behavior.
Additionally we will check \textit{type safety}: at every point of execution, all keys and values are members of predefined key- and value-sets, respectively.

\section{Hashmap for implementation}
\begin{figure}
    \begin{tla}
        ------------------------------ MODULE hashmap ------------------------------
(*****************************************************************************)
(*This module describes a hashmap to be used for testing with Shalev et al.'s*)
(*split-ordered list implementation of the data structure                    *)
(*****************************************************************************)

EXTENDS Integers

CONSTANTS NULL, PossibleKeys, PossibleValues

VARIABLES keys, map


(*******************************************************)
(*Initial state has empty map and no keys              *)
(*******************************************************)

HashmapInit ==  /\ keys = {}
                /\ map = [k \in PossibleKeys |-> NULL]

(*******************************************************)
(*Insert changes exactly one mapping of the hashmap    *)
(*and adds one key to the set of keys                  *)
(*******************************************************)
Insert ==   \exists k \in PossibleKeys :
                \exists v \in PossibleValues :
                    /\ keys' = keys \union {k}
                    /\ map' = [map EXCEPT ![k] = v]

(*******************************************************)
(*Remove sets exactly one mapping to NULL              *)
(*******************************************************)
Remove ==   \exists k \in PossibleKeys :
                /\ keys' = keys \ {k}
                /\ map' = [map EXCEPT ![k] = NULL]

(*******************************************************)
(*Find returns the value stored in the map with key k  *)
(*******************************************************)
Find(k) == map[k]

(*******************************************************)
(*Next is either an insert, a remove or a find         *)
(*Find(k) returns NULL if not in map, otherwise a value*)
(*******************************************************)          
HashmapNext ==  \/ Insert
                \/ Remove


(*******************************************************)
(*TypeOK asserts all keys and values are of the right type*)
(*******************************************************) 
TypeOK ==   \forall k \in keys :
                /\ k \in PossibleKeys
                /\ map[k] \in PossibleValues

(*******************************************************)
(*KeyHasValue asserts that every key is mapped to a value*)
(*******************************************************) 
KeyHasValue == \forall k \in keys : ~(map[k] = NULL)

(*******************************************************)
(*The hashmap specification as a temporal formula      *)
(*******************************************************) 
HashmapSpec == HashmapInit /\ [][HashmapNext]_<<keys, map>>
=============================================================================
    \end{tla}
\begin{tlatex}
\@x{\@s{32.8}}\moduleLeftDash\@xx{ {\MODULE} hashmap}\moduleRightDash\@xx{}%
\begin{lcom}{0}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
This module describes a hashmap to be used for testing with Shalev et al.'s
 split-ordered list implementation of the data structure                    
\end{cpar}%
\end{lcom}%
\@pvspace{8.0pt}%
\@x{ {\EXTENDS} Integers}%
\@pvspace{8.0pt}%
\@x{ {\CONSTANTS} NULL ,\, PossibleKeys ,\, PossibleValues}%
\@pvspace{8.0pt}%
\@x{ {\VARIABLES} keys ,\, map}%
\@pvspace{16.0pt}%
\begin{lcom}{0}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Initial state has empty map and no keys              
\end{cpar}%
\end{lcom}%
\@pvspace{8.0pt}%
\@x{ HashmapInit \.{\defeq}\@s{4.1} \.{\land} keys \.{=} \{ \}}%
\@x{\@s{4.1} \.{\land} map \.{=} [ k \.{\in} PossibleKeys \.{\mapsto} NULL ]}%
\@pvspace{8.0pt}%
\begin{lcom}{0}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Insert changes exactly one mapping of the hashmap    
 and adds one key to the set of keys                  
\end{cpar}%
\end{lcom}%
\@x{ Insert \.{\defeq}\@s{8.2} \exists\, k \.{\in} PossibleKeys \.{:}}%
\@x{\@s{24.59} \exists\, v \.{\in} PossibleValues \.{:}}%
\@x{\@s{41.0} \.{\land} keys \.{'} \.{=} keys \.{\cup} \{ k \}}%
 \@x{\@s{41.0} \.{\land} map \.{'} \.{=} [ map {\EXCEPT} {\bang} [ k ] \.{=} v
 ]}%
\@pvspace{8.0pt}%
\begin{lcom}{0}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Remove sets exactly one mapping to NULL              
\end{cpar}%
\end{lcom}%
\@x{ Remove \.{\defeq}\@s{8.2} \exists\, k \.{\in} PossibleKeys \.{:}}%
\@x{\@s{24.59} \.{\land} keys \.{'} \.{=} keys \.{\,\backslash\,} \{ k \}}%
 \@x{\@s{24.59} \.{\land} map \.{'} \.{=} [ map {\EXCEPT} {\bang} [ k ] \.{=}
 NULL ]}%
\@pvspace{8.0pt}%
\begin{lcom}{0}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Find returns the value stored in the map with key k  
\end{cpar}%
\end{lcom}%
\@x{ Find ( k ) \.{\defeq} map [ k ]}%
\@pvspace{8.0pt}%
\begin{lcom}{0}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Next is either an insert, a remove or a find         
 Find(k) returns NULL if not in map, otherwise a value
\end{cpar}%
\end{lcom}%
\@x{ HashmapNext \.{\defeq}\@s{4.1} \.{\lor} Insert}%
\@x{\@s{4.1} \.{\lor} Remove}%
\@pvspace{16.0pt}%
\begin{lcom}{0}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
TypeOK asserts all keys and values are of the right type
\end{cpar}%
\end{lcom}%
\@x{ TypeOK \.{\defeq}\@s{8.2} \forall\, k \.{\in} keys \.{:}}%
\@x{\@s{24.59} \.{\land} k \.{\in} PossibleKeys}%
\@x{\@s{24.59} \.{\land} map [ k ] \.{\in} PossibleValues}%
\@pvspace{8.0pt}%
\begin{lcom}{0}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
KeyHasValue asserts that every key is mapped to a value
\end{cpar}%
\end{lcom}%
 \@x{ KeyHasValue \.{\defeq} \forall\, k \.{\in} keys \.{:} {\lnot} ( map [ k
 ] \.{=} NULL )}%
\@pvspace{8.0pt}%
\begin{lcom}{0}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
The hashmap specification as a temporal formula      
\end{cpar}%
\end{lcom}%
 \@x{ HashmapSpec \.{\defeq} HashmapInit \.{\land} {\Box} [ HashmapNext ]_{
 {\langle} keys ,\, map {\rangle}}}%
\@x{}\bottombar\@xx{}%
\end{tlatex}
    \caption{The hashmap specification}
    \label{fig:hashmap-spec}
\end{figure}
The generic hashmap specification seen in \autoref{fig:hashmap-spec} describes how a hashmap \textit{should} behave. It maintains a set of keys \textit{keys} and a mapping \textit{map} from keys to values. Additionally it describes two operations: insert and remove.
\\\\
Insert adds a key to the set of keys and changes one entry in the map. The key is added with a set union, which preserves key uniqueness. Updating the map uses the \texttt{EXCEPT} construct, to say "the map is the same except the new key maps to the new value". The insert action is always enabled since there will always be keys and values in PossibleKeys and PossibleValues.
\\\\
Remove removes one key from the set and changes its mapping to NULL. This is done through the set difference operator and the same \texttt{EXCEPT} construct as in insert. Both operations are idempotent, so attempting to remove a key that is not in the map will result in no change.
\\\\
The specification also contains temporal formulae that specify correct behavior. These have been checked with the TLC model checker [insert results here maybe?].
\\\\
Finally the entire hashmap is described by \textit{HashmapSpec}. This temporal formula states that the hashmap specification is fulfilled if the initial state fulfills \textit{HashmapInit} and each action fulfills \textit{HashmapNext}. This formula is used to prove a more complex specificaiton implements the semantics of Hashmap.
\\\\
The theorem $SOSpec \Rightarrow HashmapSpec$ states that any program following the split-order specification also follows the hashmap specification. Hence, any program following the split-order specification is a correct implementation of a hashmap. It is tested by checking the property \textit{HashmapSpec} in a model checking SOSpec.

\section{Split-ordered List}
\begin{figure}
    \begin{tla}
        EXTENDS Integers

        CONSTANTS NULL, PossibleKeys, PossibleValues, LoadFactor, MaxSize

        VARIABLES keys, list, buckets, size, count

        ASSUME
            /\ PossibleKeys \subseteq 0..15
            /\ NULL \notin PossibleKeys
            /\ NULL \notin PossibleValues

        SOInit == /\ keys    =  {}
                  /\ list    =  [n \in 0..255 |-> IF n = 0 THEN 0 ELSE NULL]
                  /\ buckets =  [m \in PossibleKeys |-> IF m = 0 THEN 0 ELSE NULL]
                  /\ size    =  1
                  /\ count   =  0
    \end{tla}
\begin{tlatex}
\@x{\@s{32.8} {\EXTENDS} Integers}%
\@pvspace{8.0pt}%
 \@x{\@s{32.8} {\CONSTANTS} NULL ,\, PossibleKeys ,\, PossibleValues ,\,
 LoadFactor ,\, MaxSize}%
\@pvspace{8.0pt}%
\@x{\@s{32.8} {\VARIABLES} keys ,\, list ,\, buckets ,\, size ,\, count}%
\@pvspace{8.0pt}%
\@x{\@s{32.8} {\ASSUME}}%
\@x{\@s{49.19} \.{\land} PossibleKeys \.{\subseteq} 0 \.{\dotdot} 15}%
\@x{\@s{49.19} \.{\land} NULL \.{\notin} PossibleKeys}%
\@x{\@s{49.19} \.{\land} NULL \.{\notin} PossibleValues}%
\@pvspace{8.0pt}%
\@x{\@s{32.8} SOInit \.{\defeq} \.{\land} keys\@s{12.29} \.{=}\@s{4.1} \{ \}}%
 \@x{\@s{32.8} \.{\land} list\@s{12.29} \.{=}\@s{4.09} [ n \.{\in} 0
 \.{\dotdot} 255 \.{\mapsto} {\IF} n \.{=} 0 \.{\THEN} 0 \.{\ELSE} NULL ]}%
 \@x{\@s{32.8} \.{\land} buckets\@s{12.29} \.{=}\@s{4.1} [ m \.{\in}
 PossibleKeys \.{\mapsto} {\IF} m \.{=} 0 \.{\THEN} 0 \.{\ELSE} NULL ]}%
\@x{\@s{32.8} \.{\land} size\@s{12.29} \.{=}\@s{4.1} 1}%
\@x{\@s{32.8} \.{\land} count\@s{12.29} \.{=}\@s{4.1} 0}%
\end{tlatex}
    \caption{Initial state of the split-order map}
    \label{fig:split-order-init}
\end{figure}

\begin{figure}
    \begin{tla}
        ListInsert(k, v) == IF list[k] = NULL
                        THEN list' = [list EXCEPT ![k] = v] /\ count' = count + 1
                        ELSE UNCHANGED <<list, count>>
    \end{tla}
\begin{tlatex}
\@x{\@s{32.8} ListInsert ( k ,\, v ) \.{\defeq} {\IF} list [ k ] \.{=} NULL}%
 \@x{\@s{36.89} \.{\THEN} list \.{'} \.{=} [ list {\EXCEPT} {\bang} [ k ]
 \.{=} v ] \.{\land} count \.{'} \.{=} count \.{+} 1}%
\@x{\@s{36.89} \.{\ELSE} {\UNCHANGED} {\langle} list ,\, count {\rangle}}%
\end{tlatex}
    \caption{Insertion into "list"}
    \label{fig:list-insert}
\end{figure}

The split-order specification consists of three main parts. The necessary data structures (list, buckets), operations on the map, and an abstract specification tying them together. This section provides examples and explains the structure of each. The full specification can be found in \autoref{appendix:specification}.
\\\\
\autoref{fig:split-order-init} shows the beginning of the specification. The sets of possible keys and values, as well as the maximum size and load factor of the map are parameters of the model. The variables for a list and buckets are set up, as well as the set of keys, and variables storing the number of entries and current number of buckets.\\
Their initial values are set such that there is one bucket pointing to the dummy node with 0 as its key and the set of real keys is empty.
\\\\
\begin{figure}
    \begin{tla}
        BucketInsert(k, v) ==
                    (*Either a bucket needs to be initialized*)
                    \/  /\ buckets[k % size] = NULL
                        /\ BucketInit(k % size)
                        /\ ListInsert(SORegularKey(k), v)
                        /\ keys' = keys \union {k}                      
                      (*Or the bucket is already initialized*)
                    \/  /\ buckets[k % size] /= NULL
                        /\ ListInsert(SORegularKey(k), v)
                        /\ keys' = keys \union {k}
                        /\ UNCHANGED <<buckets>>
    \end{tla}
\begin{tlatex}
\@x{\@s{32.8} BucketInsert ( k ,\, v ) \.{\defeq}}%
\@x{\@s{32.8}}%
\@y{%
 Either a bucket needs to be initialized
}%
\@xx{}%
 \@x{\@s{32.8} \.{\lor}\@s{4.1} \.{\land} buckets [ k \.{\%} size ] \.{=}
 NULL}%
\@x{\@s{36.89} \.{\land} BucketInit ( k \.{\%} size )}%
\@x{\@s{36.89} \.{\land} ListInsert ( SORegularKey ( k ) ,\, v )}%
\@x{\@s{36.89} \.{\land} keys \.{'} \.{=} keys \.{\cup} \{ k \}}%
\@x{\@s{41.0}}%
\@y{%
 Or the bucket is already initialized
}%
\@xx{}%
 \@x{\@s{32.8} \.{\lor}\@s{4.1} \.{\land} buckets [ k \.{\%} size ] \.{\neq}
 NULL}%
\@x{\@s{36.89} \.{\land} ListInsert ( SORegularKey ( k ) ,\, v )}%
\@x{\@s{36.89} \.{\land} keys \.{'} \.{=} keys \.{\cup} \{ k \}}%
\@x{\@s{36.89} \.{\land} {\UNCHANGED} {\langle} buckets {\rangle}}%
\end{tlatex}
\caption{Inserting a key, value pair into the map}
\label{fig:bucket-insert}
\end{figure}

\begin{figure}
    \begin{tla}
        RECURSIVE BucketInit(_)
        BucketInit(b) == IF buckets[Parent(b)] = NULL /\ Parent(b) /= 0
                            THEN BucketInit(Parent(b))
                            ELSE buckets' = [buckets EXCEPT ![b] = SODummyKey(b)]
    \end{tla}
\begin{tlatex}
\@x{\@s{32.8} {\RECURSIVE} BucketInit ( \_ )}%
 \@x{\@s{32.8} BucketInit ( b ) \.{\defeq} {\IF} buckets [ Parent ( b ) ]
 \.{=} NULL \.{\land} Parent ( b ) \.{\neq} 0}%
\@x{\@s{32.8} \.{\THEN} BucketInit ( Parent ( b ) )}%
 \@x{\@s{32.8} \.{\ELSE} buckets \.{'} \.{=} [ buckets {\EXCEPT} {\bang} [ b ]
 \.{=} SODummyKey ( b ) ]}%
\end{tlatex}
    \caption{Initializing a bucket}
    \label{fig:bucket-init}
\end{figure}

The most involved operation is map insertion. This is done in the \textit{BucketInsert} action shown in \autoref{fig:bucket-insert}. \textit{BucketInserts} specifies the disjunction of having to initialize a bucket or inserting without changing buckets. In either case, a key is added to the set of keys and a node is inserted in the list.\\
\textit{BucketInit}, shown in \autoref{fig:bucket-init} is a recursive function initializing a pucket and any of its unitialized parent buckets. The parent of a bucket is a dummy node defined such that it comes before the bucket both in the regular order and the split-order of keys.\\
Inserting into a list is done by \textit{ListInsert} which inserts a node if there is not already one with the same key and otherwise does nothing. The list is represented as a mapping from split-order keys to values, preserving the order of nodes.

\begin{figure}
    \begin{tla}
        (*Lookup table for bit-reversed keys with MSB set*)
        SORegularKey(k) ==  CASE k = 0 -> 1
                            [] k = 1 -> 9
                            [] k = 2 -> 5
                            [] k = 3 -> 13
                            [] k = 4 -> 3
                            [] k = 5 -> 11
                            [] k = 6 -> 7
                            [] k = 7 -> 15
                            [] k = 8 -> 1
                            [] k = 9 -> 9
                            [] k = 10 -> 5
                            [] k = 11 -> 13
                            [] k = 12 -> 3
                            [] k = 13 -> 11
                            [] k = 14 -> 7
                            [] k = 15 -> 15
    \end{tla}
\begin{tlatex}
\@x{\@s{32.8}}%
\@y{%
 Lookup table for bit-reversed keys with MSB set
}%
\@xx{}%
 \@x{\@s{32.8} SORegularKey ( k ) \.{\defeq}\@s{4.1} {\CASE} k \.{=} 0
 \.{\rightarrow} 1}%
\@x{\@s{36.89} {\Box} k \.{=} 1 \.{\rightarrow} 9}%
\@x{\@s{36.89} {\Box} k \.{=} 2 \.{\rightarrow} 5}%
\@x{\@s{36.89} {\Box} k \.{=} 3 \.{\rightarrow} 13}%
\@x{\@s{36.89} {\Box} k \.{=} 4 \.{\rightarrow} 3}%
\@x{\@s{36.89} {\Box} k \.{=} 5 \.{\rightarrow} 11}%
\@x{\@s{36.89} {\Box} k \.{=} 6 \.{\rightarrow} 7}%
\@x{\@s{36.89} {\Box} k \.{=} 7 \.{\rightarrow} 15}%
\@x{\@s{36.89} {\Box} k \.{=} 8 \.{\rightarrow} 1}%
\@x{\@s{36.89} {\Box} k \.{=} 9 \.{\rightarrow} 9}%
\@x{\@s{36.89} {\Box} k \.{=} 10 \.{\rightarrow} 5}%
\@x{\@s{36.89} {\Box} k \.{=} 11 \.{\rightarrow} 13}%
\@x{\@s{36.89} {\Box} k \.{=} 12 \.{\rightarrow} 3}%
\@x{\@s{36.89} {\Box} k \.{=} 13 \.{\rightarrow} 11}%
\@x{\@s{36.89} {\Box} k \.{=} 14 \.{\rightarrow} 7}%
\@x{\@s{36.89} {\Box} k \.{=} 15 \.{\rightarrow} 15}%
\end{tlatex}
    \caption{The regular key lookup table}
    \label{fig:key-lookup}
\end{figure}

Keys and parent nodes are computed by Shalev et al. through bit operations and reversal. Such low-level operations are not available to us, so both are done with lookup tables. The lookup table for regular split-order keys is shown in \autoref{fig:key-lookup}.

\begin{figure}
    \begin{tla}
        SOInsert == /\  \E k \in PossibleKeys :
                    \E v \in PossibleValues :
                        BucketInsert(k, v)
            /\ UNCHANGED size

SORemove == /\  \E k \in PossibleKeys :
                    BucketRemove(k)
            /\ UNCHANGED size

SONext ==   \/ SOInsert
            \/ SORemove
            \/ BucketGrow

SOSpec == SOInit /\ [][SONext]_<<keys, list, buckets, size, count>>

\end{tla}
\begin{tlatex}
 \@x{\@s{32.8} SOInsert \.{\defeq} \.{\land}\@s{4.1} \E\, k \.{\in}
 PossibleKeys \.{:}}%
\@x{\@s{32.8} \E\, v \.{\in} PossibleValues \.{:}}%
\@x{\@s{36.89} BucketInsert ( k ,\, v )}%
\@x{\@s{49.19} \.{\land} {\UNCHANGED} size}%
\@pvspace{8.0pt}%
 \@x{ SORemove \.{\defeq}\@s{49.19} \.{\land}\@s{4.1} \E\, k \.{\in}
 PossibleKeys \.{:}}%
\@x{\@s{57.39} BucketRemove ( k )}%
\@x{\@s{49.19} \.{\land} {\UNCHANGED} size}%
\@pvspace{8.0pt}%
\@x{ SONext \.{\defeq}\@s{8.2} \.{\lor} SOInsert}%
\@x{\@s{8.2} \.{\lor} SORemove}%
\@x{\@s{8.2} \.{\lor} BucketGrow}%
\@pvspace{8.0pt}%
 \@x{ SOSpec \.{\defeq} SOInit \.{\land} {\Box} [ SONext ]_{ {\langle} keys
 ,\, list ,\, buckets ,\, size ,\, count {\rangle}}}%
\@pvspace{8.0pt}%
\end{tlatex}
\caption{The highest-level specification of split-order map}
\label{fig:SO-spec}
\end{figure}

Finally, the high-order functioning of the map is specified through \textit{SONext}. \textit{SONext} specifices that in each step either a key-value pair is inserted, a key is removed, or the bucket array grows in size. \textit{SOSpec} then defines the total behaviour of the map to be a sequence of states that initially satisfies \textit{SOInit} and satisfies \textit{SONext} in all steps.

\begin{figure}
    \begin{tla}
        (*********)
(*A refinement mapping of the hashmap spec with the map defined by the SOFind action*)
(***********)
INSTANCE hashmap WITH map <- [k \in PossibleKeys |-> SOFind(k)]
(********************************)
(*Split-order implements hashmap*)
(********************************)
THEOREM SOSpec => HashmapSpec
    \end{tla}
\begin{tlatex}
\@x{}%
\@y{%
 A refinement mapping of the hashmap spec with the map defined by the SOFind
 action
}%
\@xx{}%
 \@x{ {\INSTANCE} hashmap {\WITH} map \.{\leftarrow} [ k \.{\in} PossibleKeys
 \.{\mapsto} SOFind ( k ) ]}%
\begin{lcom}{0}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Split-order implements hashmap
\end{cpar}%
\end{lcom}%
\@x{ {\THEOREM} SOSpec \.{\implies} HashmapSpec}%
\end{tlatex}
\caption{Implementation of hashmap by split-order map}
\label{fig:implementation}
\end{figure}

In order to show the implementation of hashmap by SplitOrder, a refinement mapping is defined. This is a way to describe the behavior of SplitOrder in terms of the variables of hashmap. The mapping replaces the variable \textit{map} in hashmap with the \textit{SOFind} operation defined in SplitOrder. It is necessary to perform such a replacement because SplitOrder does not define a simple map from keys to values, but rather uses the structure of buckets and list nodes to find the correct node. The refinement mapping ans subsequent implementation theorem can be seen in \autoref{fig:implementation}.

\begin{figure}
\begin{tla}
(*************************************)
(*Type correctness of keys and values*)
(*************************************)
SOTypeOK == \forall k \in keys :
                /\ k \in PossibleKeys
                /\ list[SORegularKey(k)] \in PossibleValues

(***************************************************************)
(*Each bucket is either uninitialized or points to a dummy node*)
(***************************************************************)
SOBucketOK == \forall n \in 0..size :
                    \/ buckets[n] = NULL
                    \/ buckets[n] = SODummyKey(n)
\end{tla}
\begin{tlatex}
\begin{lcom}{0}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Type correctness of keys and values
\end{cpar}%
\end{lcom}%
\@x{ SOTypeOK \.{\defeq} \forall\, k \.{\in} keys \.{:}}%
\@x{\@s{16.4} \.{\land} k \.{\in} PossibleKeys}%
\@x{\@s{16.4} \.{\land} list [ SORegularKey ( k ) ] \.{\in} PossibleValues}%
\@pvspace{8.0pt}%
\begin{lcom}{0}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Each bucket is either uninitialized or points to a dummy node
\end{cpar}%
\end{lcom}%
\@x{ SOBucketOK \.{\defeq} \forall\, n \.{\in} 0 \.{\dotdot} size \.{:}}%
\@x{\@s{24.59} \.{\lor} buckets [ n ] \.{=} NULL}%
\@x{\@s{24.59} \.{\lor} buckets [ n ] \.{=} SODummyKey ( n )}%
\end{tlatex}
\caption{Invariants in TLA^+}
\label{fig:invariants}
\end{figure}


\chapter{Results}\label{ch:results}
\section{Discussion}\label{sec:discussion}
[some sort of introduction]
\\\\
% I learned a lot, but the output does not prove anything about concurrent correctness
% structure of the spec follows the structure of the implementation, which does not lend itself to logical state transitions.
% should have made a more conscious decision about step size
% "correct" thinking: what constitutes the state? when does that change?
% implementation (the proof kind) looked to be elegant, but introduced unwanted complexity
The first attempt at writing a specification served as a learning tool for the TLA+ language, but did not produce a useful specification for the purpose of proving correctness in a concurrent setting. The structure of the specification was based on the structure of an implementation in that actions correspond to the \textit{insert}, \textit{delete} and \textit{bucket init} functions in Shalev et al.'s pseudocode. Such a structure brings with it the implicit assumption that the functions are linearizable since one action corresponds to one state transition.
\\\\
The attempt to show correctness through implementation also posed difficulties. The idealized hashmap specification proved impossible to directly map to the split-order specification because a \textit{find} operation in may result in a bucket needing to be initialized and thus can not be used directly in a refinement mapping. Instead we introduced a set of auxiliary variables~\cite{Lamport2019a} and maintained an idealized map that was updated in atomic steps to implement the Hashmap specification. The updates loosely correspond to a \textit{find} operation and the correctness of this idealized map does seem to correspond to a correct underlying structure (albeit at an earlier state), but introduces additional complexity to the spec and does not resolve the underlying issue of assumed linearizability.

\backmatter
\printbibliography{}
\appendix
\addcontentsline{toc}{chapter}{Appendix}
% \addtocontents{toc}{\protect\contentsline{chapter}{Appendix:}{}}
\chapter{Split-order Specification}\label{appendix:specification}
    \begin{tla}
        --------------------------------- MODULE SplitOrder ---------------------------------
        (*************************************************************************************)
        (*This module implements a hashmap using Shalev et al.'s split-ordered list structure*)
        (*************************************************************************************)
        
        EXTENDS Integers
        
        CONSTANTS NULL, PossibleKeys, PossibleValues, LoadFactor, MaxSize
        
        VARIABLES keys, list, buckets, size, count
        
        ASSUME
        /\ PossibleKeys \subseteq 0..15
        /\ NULL \notin PossibleKeys
        /\ NULL \notin PossibleValues
        
        (****************************************************)
        (*The Init for split-order                          *)
        (*keys is initially empty                           *)
        (*the map maps every possible key to NULL           *)
        (*The list initially contains only the 0 dummy node *)
        (****************************************************)
SOInit ==   /\ keys = {}
/\ list = [n \in 0..255 |-> IF n = 0 THEN 0 ELSE NULL]
/\ buckets = [m \in PossibleKeys |-> IF m = 0 THEN 0 ELSE NULL]
/\ size = 1
/\ count = 0

(*Lookup table for bit-reversed keys with MSB set*)
SORegularKey(k) ==  CASE k = 0 -> 1
[] k = 1 -> 9
[] k = 2 -> 5
[] k = 3 -> 13
[] k = 4 -> 3
[] k = 5 -> 11
[] k = 6 -> 7
[] k = 7 -> 15
[] k = 8 -> 1
[] k = 9 -> 9
[] k = 10 -> 5
[] k = 11 -> 13
[] k = 12 -> 3
[] k = 13 -> 11
[] k = 14 -> 7
[] k = 15 -> 15

(*Lookup table for bit-reversed keys*)
SODummyKey(k) ==    CASE k = 0 -> 0
[] k = 1 -> 8
[] k = 2 -> 4
[] k = 3 -> 12
[] k = 4 -> 2
[] k = 5 -> 10
[] k = 6 -> 6
[] k = 7 -> 14
[] k = 8 -> 1
[] k = 9 -> 9
[] k = 10 -> 5
[] k = 11 -> 13
[] k = 12 -> 3
[] k = 13 -> 11
[] k = 14 -> 7
                     [] k = 15 -> 15
                     
                     (*Lookup table for parent buckets*)
                     Parent(b) ==    CASE b = 0 -> 0
                     [] b = 1 -> 0
                     [] b = 2 -> 0
                     [] b = 3 -> 1
                     [] b = 4 -> 0
                     [] b = 5 -> 1
                     [] b = 6 -> 2
                     [] b = 7 -> 3
                     [] b = 8 -> 0
                     [] b = 9 -> 1
                     [] b = 10 -> 2
                     [] b = 11 -> 3
                     [] b = 12 -> 8
                     [] b = 13 -> 5
                     [] b = 14 -> 6
                     [] b = 15 -> 7
                     
                     
                     (**********************************)
                     (*Inserting into the "linked list"*)
                     (**********************************)
                     ListInsert(k, v) == IF list[k] = NULL
                     THEN list' = [list EXCEPT ![k] = v] /\ count' = count + 1
                     ELSE UNCHANGED <<list, count>>
                     
                     (*********************************)
                     (*Removing from the "linked list"*)
                     (*********************************)
                     ListRemove(k) == IF list[k] = NULL
                     THEN UNCHANGED <<list, count>>
                     ELSE list' = [list EXCEPT ![k] = NULL] /\ count' = count - 1
                     
                     (*********************************)
                     (*Recursively initializes buckets*)
                     (*********************************)
                     RECURSIVE BucketInit(_)
                     BucketInit(b) == IF buckets[Parent(b)] = NULL /\ Parent(b) /= 0
                     THEN BucketInit(Parent(b))
                     ELSE buckets' = [buckets EXCEPT ![b] = SODummyKey(b)]
                     
                     (********************************************************)
                     (*Find the value of the key k in the bucket b           *)
                     (*Results in the value if b is initialized and k is in b*)
                     (********************************************************)
                     ListFind(b, k) == IF k > b /\ list[b] /= NULL THEN list[k] ELSE NULL
                     
                     (*SOFind finds a key in the map*)
                     SOFind(k) == IF buckets[k % size] = NULL
                     THEN NULL \* should initialize bucket, but also needs a "return value"
                     ELSE ListFind(buckets[k % size], k)
                     
                     Min(a, b) == IF a > b THEN b ELSE a
                     BucketGrow == IF count /= 0 /\ size \div count > LoadFactor
                     THEN size' = Min(size * 2, MaxSize) /\ UNCHANGED <<keys, list, buckets, count>> 
                     ELSE UNCHANGED <<keys, list, buckets, count, size>>
                     
                     (****************************)
                     (*Inserting into the buckets*)
                     (****************************)
                     BucketInsert(k, v) == (*Either a bucket needs to be initialized*)
                     \/  /\ buckets[k % size] = NULL
                     /\ BucketInit(k % size)
                     /\ ListInsert(SORegularKey(k), v)
                     /\ keys' = keys \union {k}                      
                     (*Or the bucket is already initialized*)
                     \/  /\ buckets[k % size] /= NULL
                     /\ ListInsert(SORegularKey(k), v)
                     /\ keys' = keys \union {k}
                     /\ UNCHANGED <<buckets>>
                     
                     (***************************)
                     (*Removing from the buckets*)
                     (***************************)                
                     BucketRemove(k) == /\ ListRemove(SORegularKey(k))
                     /\ keys' = keys \ {k}
                     /\ UNCHANGED <<buckets>>
                     
                     
                     SOInsert == /\  \E k \in PossibleKeys :
                     \E v \in PossibleValues :
                     BucketInsert(k, v)
                     /\ UNCHANGED size
                     
                     SORemove == /\  \E k \in PossibleKeys :
                     BucketRemove(k)
                     /\ UNCHANGED size
                     
                     
                     (**************************)
                     (*The Next for split order*)
                     (**************************)
                     SONext ==   \/ SOInsert
                     \/ SORemove
                     \/ BucketGrow
                     
                     (**************************)
                     (*Split-order spec        *)
                     (**************************)
                     SOSpec == SOInit /\ [][SONext]_<<keys, list, buckets, size, count>>
                     
                     
                     (*If I can get map to work as intended...*)
                     (*refinement mapping??*)
                     INSTANCE  hashmap WITH map <- [k \in PossibleKeys |-> SOFind(k)]
                     (********************************)
                     (*Split-order implements hashmap*)
                     (********************************)
                     THEOREM SOSpec => HashmapSpec
                     
                     ======================================================================================
    \end{tla}
\begin{tlatex}
 \@x{\@s{32.8}}\moduleLeftDash\@xx{ {\MODULE}
 SplitOrder}\moduleRightDash\@xx{}%
\begin{lcom}{32.8}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
 This module implements a hashmap using Shalev et al.'s split-ordered list
 structure
\end{cpar}%
\end{lcom}%
\@pvspace{8.0pt}%
\@x{\@s{32.8} {\EXTENDS} Integers}%
\@pvspace{8.0pt}%
 \@x{\@s{32.8} {\CONSTANTS} NULL ,\, PossibleKeys ,\, PossibleValues ,\,
 LoadFactor ,\, MaxSize}%
\@pvspace{8.0pt}%
\@x{\@s{32.8} {\VARIABLES} keys ,\, list ,\, buckets ,\, size ,\, count}%
\@pvspace{8.0pt}%
\@x{\@s{32.8} {\ASSUME}}%
\@x{\@s{32.8} \.{\land} PossibleKeys \.{\subseteq} 0 \.{\dotdot} 15}%
\@x{\@s{32.8} \.{\land} NULL \.{\notin} PossibleKeys}%
\@x{\@s{32.8} \.{\land} NULL \.{\notin} PossibleValues}%
\@pvspace{8.0pt}%
\begin{lcom}{32.8}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
The Init for split-order                          
 keys is initially empty                           
 the map maps every possible key to NULL           
 The list initially contains only the 0 dummy node 
\end{cpar}%
\end{lcom}%
\@x{ SOInit \.{\defeq}\@s{8.2} \.{\land} keys \.{=} \{ \}}%
 \@x{ \.{\land} list \.{=} [ n \.{\in} 0 \.{\dotdot} 255 \.{\mapsto} {\IF} n
 \.{=} 0 \.{\THEN} 0 \.{\ELSE} NULL ]}%
 \@x{ \.{\land} buckets \.{=} [ m \.{\in} PossibleKeys \.{\mapsto} {\IF} m
 \.{=} 0 \.{\THEN} 0 \.{\ELSE} NULL ]}%
\@x{ \.{\land} size \.{=} 1}%
\@x{ \.{\land} count \.{=} 0}%
\@pvspace{8.0pt}%
\@x{}%
\@y{%
 Lookup table for bit-reversed keys with MSB set
}%
\@xx{}%
 \@x{ SORegularKey ( k ) \.{\defeq}\@s{4.1} {\CASE} k \.{=} 0 \.{\rightarrow}
 1}%
\@x{ {\Box} k \.{=} 1 \.{\rightarrow} 9}%
\@x{ {\Box} k \.{=} 2 \.{\rightarrow} 5}%
\@x{ {\Box} k \.{=} 3 \.{\rightarrow} 13}%
\@x{ {\Box} k \.{=} 4 \.{\rightarrow} 3}%
\@x{ {\Box} k \.{=} 5 \.{\rightarrow} 11}%
\@x{ {\Box} k \.{=} 6 \.{\rightarrow} 7}%
\@x{ {\Box} k \.{=} 7 \.{\rightarrow} 15}%
\@x{ {\Box} k \.{=} 8 \.{\rightarrow} 1}%
\@x{ {\Box} k \.{=} 9 \.{\rightarrow} 9}%
\@x{ {\Box} k \.{=} 10 \.{\rightarrow} 5}%
\@x{ {\Box} k \.{=} 11 \.{\rightarrow} 13}%
\@x{ {\Box} k \.{=} 12 \.{\rightarrow} 3}%
\@x{ {\Box} k \.{=} 13 \.{\rightarrow} 11}%
\@x{ {\Box} k \.{=} 14 \.{\rightarrow} 7}%
\@x{ {\Box} k \.{=} 15 \.{\rightarrow} 15}%
\@pvspace{8.0pt}%
\@x{}%
\@y{%
 Lookup table for bit-reversed keys
}%
\@xx{}%
 \@x{ SODummyKey ( k ) \.{\defeq}\@s{12.29} {\CASE} k \.{=} 0 \.{\rightarrow}
 0}%
\@x{ {\Box} k \.{=} 1 \.{\rightarrow} 8}%
\@x{ {\Box} k \.{=} 2 \.{\rightarrow} 4}%
\@x{ {\Box} k \.{=} 3 \.{\rightarrow} 12}%
\@x{ {\Box} k \.{=} 4 \.{\rightarrow} 2}%
\@x{ {\Box} k \.{=} 5 \.{\rightarrow} 10}%
\@x{ {\Box} k \.{=} 6 \.{\rightarrow} 6}%
\@x{ {\Box} k \.{=} 7 \.{\rightarrow} 14}%
\@x{ {\Box} k \.{=} 8 \.{\rightarrow} 1}%
\@x{ {\Box} k \.{=} 9 \.{\rightarrow} 9}%
\@x{ {\Box} k \.{=} 10 \.{\rightarrow} 5}%
\@x{ {\Box} k \.{=} 11 \.{\rightarrow} 13}%
\@x{ {\Box} k \.{=} 12 \.{\rightarrow} 3}%
\@x{ {\Box} k \.{=} 13 \.{\rightarrow} 11}%
\@x{ {\Box} k \.{=} 14 \.{\rightarrow} 7}%
\@x{\@s{32.8} {\Box} k \.{=} 15 \.{\rightarrow} 15}%
\@pvspace{8.0pt}%
\@x{\@s{32.8}}%
\@y{%
 Lookup table for parent buckets
}%
\@xx{}%
 \@x{\@s{32.8} Parent ( b ) \.{\defeq}\@s{12.29} {\CASE} b \.{=} 0
 \.{\rightarrow} 0}%
\@x{\@s{32.8} {\Box} b \.{=} 1 \.{\rightarrow} 0}%
\@x{\@s{32.8} {\Box} b \.{=} 2 \.{\rightarrow} 0}%
\@x{\@s{32.8} {\Box} b \.{=} 3 \.{\rightarrow} 1}%
\@x{\@s{32.8} {\Box} b \.{=} 4 \.{\rightarrow} 0}%
\@x{\@s{32.8} {\Box} b \.{=} 5 \.{\rightarrow} 1}%
\@x{\@s{32.8} {\Box} b \.{=} 6 \.{\rightarrow} 2}%
\@x{\@s{32.8} {\Box} b \.{=} 7 \.{\rightarrow} 3}%
\@x{\@s{32.8} {\Box} b \.{=} 8 \.{\rightarrow} 0}%
\@x{\@s{32.8} {\Box} b \.{=} 9 \.{\rightarrow} 1}%
\@x{\@s{32.8} {\Box} b \.{=} 10 \.{\rightarrow} 2}%
\@x{\@s{32.8} {\Box} b \.{=} 11 \.{\rightarrow} 3}%
\@x{\@s{32.8} {\Box} b \.{=} 12 \.{\rightarrow} 8}%
\@x{\@s{32.8} {\Box} b \.{=} 13 \.{\rightarrow} 5}%
\@x{\@s{32.8} {\Box} b \.{=} 14 \.{\rightarrow} 6}%
\@x{\@s{32.8} {\Box} b \.{=} 15 \.{\rightarrow} 7}%
\@pvspace{16.0pt}%
\begin{lcom}{32.8}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Inserting into the "linked list"
\end{cpar}%
\end{lcom}%
\@x{\@s{32.8} ListInsert ( k ,\, v ) \.{\defeq} {\IF} list [ k ] \.{=} NULL}%
 \@x{\@s{32.8} \.{\THEN} list \.{'} \.{=} [ list {\EXCEPT} {\bang} [ k ] \.{=}
 v ] \.{\land} count \.{'} \.{=} count \.{+} 1}%
\@x{\@s{32.8} \.{\ELSE} {\UNCHANGED} {\langle} list ,\, count {\rangle}}%
\@pvspace{8.0pt}%
\begin{lcom}{32.8}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Removing from the "linked list"
\end{cpar}%
\end{lcom}%
\@x{\@s{32.8} ListRemove ( k ) \.{\defeq} {\IF} list [ k ] \.{=} NULL}%
\@x{\@s{32.8} \.{\THEN} {\UNCHANGED} {\langle} list ,\, count {\rangle}}%
 \@x{\@s{32.8} \.{\ELSE} list \.{'} \.{=} [ list {\EXCEPT} {\bang} [ k ] \.{=}
 NULL ] \.{\land} count \.{'} \.{=} count \.{-} 1}%
\@pvspace{8.0pt}%
\begin{lcom}{32.8}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Recursively initializes buckets
\end{cpar}%
\end{lcom}%
\@x{\@s{32.8} {\RECURSIVE} BucketInit ( \_ )}%
 \@x{\@s{32.8} BucketInit ( b ) \.{\defeq} {\IF} buckets [ Parent ( b ) ]
 \.{=} NULL \.{\land} Parent ( b ) \.{\neq} 0}%
\@x{\@s{32.8} \.{\THEN} BucketInit ( Parent ( b ) )}%
 \@x{\@s{32.8} \.{\ELSE} buckets \.{'} \.{=} [ buckets {\EXCEPT} {\bang} [ b ]
 \.{=} SODummyKey ( b ) ]}%
\@pvspace{8.0pt}%
\begin{lcom}{32.8}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Find the value of the key k in the bucket b           
 Results in the value if b is initialized and k is in b
\end{cpar}%
\end{lcom}%
 \@x{\@s{32.8} ListFind ( b ,\, k ) \.{\defeq} {\IF} k \.{>} b \.{\land} list
 [ b ] \.{\neq} NULL \.{\THEN} list [ k ] \.{\ELSE} NULL}%
\@pvspace{8.0pt}%
\@x{\@s{32.8}}%
\@y{%
 SOFind finds a key in the map
}%
\@xx{}%
 \@x{\@s{32.8} SOFind ( k ) \.{\defeq} {\IF} buckets [ k \.{\%} size ] \.{=}
 NULL}%
\@x{\@s{32.8} \.{\THEN} NULL}%
\@y{%
  should initialize bucket, but also needs a "return value"
}%
\@xx{}%
\@x{\@s{32.8} \.{\ELSE} ListFind ( buckets [ k \.{\%} size ] ,\, k )}%
\@pvspace{8.0pt}%
 \@x{\@s{32.8} Min ( a ,\, b ) \.{\defeq} {\IF} a \.{>} b \.{\THEN} b
 \.{\ELSE} a}%
 \@x{\@s{32.8} BucketGrow \.{\defeq} {\IF} count \.{\neq} 0 \.{\land} size
 \.{\div} count \.{>} LoadFactor}%
 \@x{\@s{32.8} \.{\THEN} size \.{'} \.{=} Min ( size \.{*} 2 ,\, MaxSize )
 \.{\land} {\UNCHANGED} {\langle} keys ,\, list ,\, buckets ,\, count
 {\rangle}}%
 \@x{\@s{32.8} \.{\ELSE} {\UNCHANGED} {\langle} keys ,\, list ,\, buckets ,\,
 count ,\, size {\rangle}}%
\@pvspace{8.0pt}%
\begin{lcom}{32.8}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Inserting into the buckets
\end{cpar}%
\end{lcom}%
\@x{\@s{32.8} BucketInsert ( k ,\, v ) \.{\defeq}}%
\@y{%
 Either a bucket needs to be initialized
}%
\@xx{}%
 \@x{\@s{32.8} \.{\lor}\@s{4.1} \.{\land} buckets [ k \.{\%} size ] \.{=}
 NULL}%
\@x{\@s{32.8} \.{\land} BucketInit ( k \.{\%} size )}%
\@x{\@s{32.8} \.{\land} ListInsert ( SORegularKey ( k ) ,\, v )}%
\@x{\@s{32.8} \.{\land} keys \.{'} \.{=} keys \.{\cup} \{ k \}}%
\@x{\@s{32.8}}%
\@y{%
 Or the bucket is already initialized
}%
\@xx{}%
 \@x{\@s{32.8} \.{\lor}\@s{4.1} \.{\land} buckets [ k \.{\%} size ] \.{\neq}
 NULL}%
\@x{\@s{32.8} \.{\land} ListInsert ( SORegularKey ( k ) ,\, v )}%
\@x{\@s{32.8} \.{\land} keys \.{'} \.{=} keys \.{\cup} \{ k \}}%
\@x{\@s{32.8} \.{\land} {\UNCHANGED} {\langle} buckets {\rangle}}%
\@pvspace{8.0pt}%
\begin{lcom}{32.8}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Removing from the buckets
\end{cpar}%
\end{lcom}%
 \@x{\@s{32.8} BucketRemove ( k ) \.{\defeq} \.{\land} ListRemove (
 SORegularKey ( k ) )}%
\@x{\@s{32.8} \.{\land} keys \.{'} \.{=} keys \.{\,\backslash\,} \{ k \}}%
\@x{\@s{32.8} \.{\land} {\UNCHANGED} {\langle} buckets {\rangle}}%
\@pvspace{16.0pt}%
 \@x{\@s{32.8} SOInsert \.{\defeq} \.{\land}\@s{4.1} \E\, k \.{\in}
 PossibleKeys \.{:}}%
\@x{\@s{32.8} \E\, v \.{\in} PossibleValues \.{:}}%
\@x{\@s{32.8} BucketInsert ( k ,\, v )}%
\@x{\@s{32.8} \.{\land} {\UNCHANGED} size}%
\@pvspace{8.0pt}%
 \@x{\@s{32.8} SORemove \.{\defeq} \.{\land}\@s{4.1} \E\, k \.{\in}
 PossibleKeys \.{:}}%
\@x{\@s{32.8} BucketRemove ( k )}%
\@x{\@s{32.8} \.{\land} {\UNCHANGED} size}%
\@pvspace{16.0pt}%
\begin{lcom}{32.8}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
The Next for split order
\end{cpar}%
\end{lcom}%
\@x{\@s{32.8} SONext \.{\defeq}\@s{8.2} \.{\lor} SOInsert}%
\@x{\@s{32.8} \.{\lor} SORemove}%
\@x{\@s{32.8} \.{\lor} BucketGrow}%
\@pvspace{8.0pt}%
\begin{lcom}{32.8}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Split-order spec        
\end{cpar}%
\end{lcom}%
 \@x{\@s{32.8} SOSpec \.{\defeq} SOInit \.{\land} {\Box} [ SONext ]_{
 {\langle} keys ,\, list ,\, buckets ,\, size ,\, count {\rangle}}}%
\@pvspace{16.0pt}%
\@x{\@s{32.8}}%
\@y{%
 If I can get map to work as intended...
}%
\@xx{}%
\@x{\@s{32.8}}%
\@y{%
 refinement mapping??
}%
\@xx{}%
 \@x{\@s{32.8} {\INSTANCE}\@s{4.1} hashmap {\WITH} map \.{\leftarrow} [ k
 \.{\in} PossibleKeys \.{\mapsto} SOFind ( k ) ]}%
\begin{lcom}{32.8}%
\begin{cpar}{0}{F}{F}{0}{0}{}%
Split-order implements hashmap
\end{cpar}%
\end{lcom}%
\@x{\@s{32.8} {\THEOREM} SOSpec \.{\implies} HashmapSpec}%
\@pvspace{8.0pt}%
\@x{\@s{32.8}}\bottombar\@xx{}%
\end{tlatex}
\end{document}
